import ROOT
import os,sys
from ROOT import TGraphAsymmErrors
from ROOT import TGraphErrors
from ROOT import TColor
from array import array
from operator import truediv
import random
import math
import pandas
import numpy as np
import os.path
import glob
import logging as log
import argparse
import pdb

parser = argparse.ArgumentParser()
parser.add_argument("-b","--beginRun",help="first run to analyze [%default]",default=299918)
parser.add_argument("-e","--endRun",help="analyze stops when comes to this run [%default]",default=1000000)
parser.add_argument("-p","--parametrizeType",help="define parametrization: 1 is for extrapolation, 2 is for piece-wise function",default=1)
parser.add_argument("-v","--verbose",help="increase logging level from INFO to DEBUG",default=False,action="store_true")
parser.add_argument("-c","--writeSummaryCSV",help="produce merged CSV with all runs",default=True)
parser.add_argument("-d","--dirDQM",help="Directory to the input root files from the DQM Offline module",default="/afs/cern.ch/user/d/dwalter/cernBox/www/ZCounting/DQM-Offline-2018/")
parser.add_argument("-f","--ByLsCSV",help="ByLs csv input generated by testBril.sh",default="/eos/cms/store/group/comm_luminosity/ZCounting/brilcalcFile2018/*csv")
parser.add_argument("-g","--dirMC",help="Directory to root files for pilupe reweighting",default="/afs/cern.ch/work/x/xniu/public/CMSSW_9_2_8/src/ZCountHarvest/LookupTable/")
parser.add_argument("-t","--dirMCShape",help="Directory to root file for Z mass template",default="MCFiles/92X_norw_IsoMu27_noIso/")
parser.add_argument("-a","--dirCSV",help="where to write/store the CSV files",default="./")
parser.add_argument("-x","--dirEff",help="where to write/store efficiency Plots",default="./")

args = parser.parse_args()
if args.verbose:
    log.basicConfig(format="%(levelname)s: %(message)s", level=log.DEBUG)
else:
    log.basicConfig(format="%(levelname)s: %(message)s", level=log.INFO)

########## Input configuration ##########

#ByLS csv inputs generated by testBRIL.sh 
#inFile="/afs/cern.ch/work/x/xniu/public/CMSSW_9_2_8/src/ZCountHarvest/CloneJob/2017LumiByLS_hfet_trig_PU.csv"
inFileList=glob.glob(args.ByLsCSV)
inFileList.sort(key=os.path.getmtime)
inFile=inFileList[-1]
print "The brilcalc csv file: "+str(inFile)

eosDir= args.dirDQM 

#MC inputs: to build MC*Gaussian template for efficiency fitting

mcDir= args.dirMC # "/afs/cern.ch/work/x/xniu/public/CMSSW_9_2_8/src/ZCountHarvest/LookupTable/"
mcShapeSubDir= args.dirMCShape# "MCFiles/92X_norw_IsoMu27_noIso/"

########################################

########### Constant settings ##########
secPerLS=float(23.3)
currentYear=2018
maximumLS=2500
ZperMeasurement=10000 #required number of Z per measurement
staFitChi2Th=2.      #threshold on chi2 to trigger protection mechanism
staFitEffThHi=0.999  #threshold on eff. to trigger protection mechanism
staFitEffThLo=0.95   #threshold on eff. to trigger protection mechanism

ZfpRate = 0.01		#False positive rate of Z: background events counted as Z
ZBBRate = 0.077904 	#Fraction of Z events where both muons are in barrel region
ZBERate = 0.117200	# barrel and endcap
ZEERate = 0.105541	# both endcap

########################################

########## Pileup Correctins ###########

paraType=int(args.parametrizeType)

def fGen(a1,b1,a2,b2,a3,b3,fstep,para):
    #This function generates the pileup correction function, there are two models implemented
    #  para=1: linear function 
    #  para=2: stepwise linear function
    if para==1:
        def f(x):
            return a1 + b1*x
    elif para==2:
        def f(x):
            if x < fstep:
                return a2 + b2 * x
            else:
                return a3 + b3 * x
    else:
        print("ERROR: invalid parameterization type for pilup correction function")
    return f

#Define Z efficiency correction for pileup - parameters from MC
f_ZBBEffCorr = fGen(0.00305155,0.000519427, 0.0123732 ,0.000161345, -0.0878557 ,0.00218727 , 50, paraType)
f_ZBEEffCorr = fGen(0.00585766,0.000532229, 0.00875762,0.000493846, -0.0600895 ,0.00179    , 55, paraType)
f_ZEEEffCorr = fGen(0.0114659 ,0.00048351 , 0.0160629 ,0.000296308,  0.00132398,0.000743008, 40, paraType)

########################################

#log.info("Loading C marco...")	#I think we don't need this
ROOT.gROOT.LoadMacro(os.path.dirname(os.path.realpath(__file__))+"/calculateDataEfficiency.C") #load function getZyield(...) and calculateDataEfficiency(...)

#turn off graphical output on screen
ROOT.gROOT.SetBatch(True)

log.info("Loading input byls csv...")
lumiFile=open(str(inFile))
lumiLines=lumiFile.readlines()
data=pandas.read_csv(inFile, sep=',',low_memory=False, skiprows=[0,len(lumiLines)-11,len(lumiLines)-10,len(lumiLines)-9,len(lumiLines)-8,len(lumiLines)-7,len(lumiLines)-6,len(lumiLines)-5,len(lumiLines)-4,len(lumiLines)-3,len(lumiLines)-2,len(lumiLines)-1,len(lumiLines)])
log.debug("%s",data.axes)
log.info("Loading input byls csv DONE...")
#formatting the csv
data['fill'] = pandas.to_numeric(data['#run:fill'].str.split(':',expand=True)[1])
data['run'] = pandas.to_numeric(data['#run:fill'].str.split(':',expand=True)[0])
data['ls'] = pandas.to_numeric(data['ls'].str.split(':',expand=True)[0])
data = data.drop(['#run:fill','hltpath','source'],axis=1)

if 'delivered(/ub)' in data.columns.tolist():      #convert to /pb
    data['delivered(/ub)'] = data['delivered(/ub)'].apply(lambda x:x / 1000000.)
    data['recorded(/ub)'] = data['recorded(/ub)'].apply(lambda x:x / 1000000.)
    data = data.rename(index=str, columns={'delivered(/ub)':'delivered(/pb)', 'recorded(/ub)':'recorded(/pb)' })


log.info("Looping over runs...")
for run in data.drop_duplicates('run')['run'].values:

    data_run = data.loc[data['run'] == run]

    fill = data_run.drop_duplicates('fill')['fill'].values[0]
    LSlist = data_run['ls'].values.tolist()

    if run<int(args.beginRun) or run>=int(args.endRun):
        continue
    
    #check if run was processed already
    processedRun = glob.glob(args.dirEff+'Run'+str(run))
    if len(processedRun)>0:
        print "Run "+str(run)+" was already processed, skipping and going to next run"
        continue
    

    #era split follows here:https://twiki.cern.ch/twiki/bin/viewauth/CMS/PdmV2017Analysis#DATA

    log.info("===Running Run %i",run)
    log.info("===Running Fill %i",fill)
        
    log.debug("===Setting up arrays for output csv...")
    fillarray=array('d')
    beginTime=[]
    endTime=[]
    Zrate=array('d')
    ZrateUnCorr=array('d')
    instDel=array('d')
    lumiDel=array('d')
    pileUp=array('d')
    ZyieldDel=array('d')

    ZyieldRec=array('d')
    lumiRec=array('d')
    windowarray=array('d')
    deadTime=array('d')
    beginLS=array('i')
    endLS=array('i')

    HLTeffB=array('d')
    HLTeffE=array('d')
    SITeffB=array('d')
    SITeffE=array('d')
    StaeffB=array('d')
    StaeffE=array('d')

    ZMCeff=array('d')
    ZMCeffBB=array('d')
    ZMCeffBE=array('d')
    ZMCeffEE=array('d')

    ZBBeff=array('d')
    ZBEeff=array('d')
    ZEEeff=array('d')

    nMeasurements=0
    prevStaEffB=0.98
    prevStaEffE=0.98

    log.info("===Loading input DQMIO.root file...")
    eosFileList = glob.glob(eosDir+'/*/*'+str(run)+'*root')

    if not len(eosFileList)>0:
	print "The file does not yet exist for run: "+str(run)
	continue
    else:
	eosFile=eosFileList[0]

    print "The file exists: "+str(eosFile)+" for run  "+str(run)
    log.info("===Looping over measurements...")

    while len(LSlist) > 0: #begin next measurement "m"
        log.debug("Openning DQMIO.root file: %s", eosFile)

        f1 = ROOT.TFile(eosFile)
        h_yield_Z = f1.Get("DQMData/Run "+str(run)+"/ZCounting/Run summary/Histograms/h_yield_Z")
        # number of events in each ls (which may or may not have a Z candidate)
        h_n0 = f1.Get("DQMData/Run "+str(run)+"/ZCounting/Run summary/Histograms/h_npv").ProjectionX() 
        # produce goodLSlist with ls that are used for one measurement
        # Each measurement should have more than 5000 Z's, If the last measurement is smaller, combine it
        # Note: lumisection 'n' is stored in histogram bin 'n+1'
        #   therefore we have to add 1 to the lumisection in question
        Zyield_m = 0
        n0list = []
        goodLSlist = []
        while Zyield_m < ZperMeasurement or (len(LSlist) > 0 and h_yield_Z.Integral(LSlist[0]+1,LSlist[-1]+1) < ZperMeasurement): 
            if len(LSlist) < 1:
                print("No more lumi sections in current run")
                break
            if LSlist[0] > h_yield_Z.GetNbinsX():
                log.error("======Lumi Section not stored in root file")
                break
            n0_ls = h_n0.GetBinContent(LSlist[0]+1)
            if n0_ls > 0:
                Zyield_m += h_yield_Z.GetBinContent(LSlist[0]+1)
                n0list.append(n0_ls)
                goodLSlist.append(LSlist[0])
            del LSlist[0]

        avgpu_m = sum(data_run.loc[data_run['ls'].isin(goodLSlist)]['avgpu'].values * np.array(n0list))/sum(n0list)
        recLumi_m = sum(data_run.loc[data_run['ls'].isin(goodLSlist)]['recorded(/pb)'].values)
        delLumi_m = sum(data_run.loc[data_run['ls'].isin(goodLSlist)]['delivered(/pb)'].values)
        deadtime_m = recLumi_m/delLumi_m
        timeWindow_m = len(goodLSlist) * secPerLS

        datestampLow_m = data_run.loc[data_run['ls'] == goodLSlist[0]]['time'].values[0].split(" ")
        datestampUp_m = data_run.loc[data_run['ls'] == goodLSlist[-1]]['time'].values[0].split(" ")

	dateLow_m=ROOT.TDatime(currentYear,int(datestampLow_m[0].split("/")[0]),int(datestampLow_m[0].split("/")[1]),int(datestampLow_m[1].split(":")[0]),int(datestampLow_m[1].split(":")[1]),int(datestampLow_m[1].split(":")[2]))
	dateUp_m=ROOT.TDatime(currentYear,int(datestampUp_m[0].split("/")[0]),int(datestampUp_m[0].split("/")[1]),int(datestampUp_m[1].split(":")[0]),int(datestampUp_m[1].split(":")[1]),int(datestampUp_m[1].split(":")[2]))
        
        log.debug("======beginTime: %s",dateLow_m.Convert())
        log.debug("======endTime: %s",dateUp_m.Convert())
        log.debug("======timeWindow: %f",timeWindow_m)

        HLTeffresB_m=ROOT.calculateDataEfficiency(str(eosFile),args.dirEff,str(run),nMeasurements,goodLSlist[0]+1,goodLSlist[-1]+1,avgpu_m,"HLT",0,0,0,0,0,recLumi_m)
        HLTeffresE_m=ROOT.calculateDataEfficiency(str(eosFile),args.dirEff,str(run),nMeasurements,goodLSlist[0]+1,goodLSlist[-1]+1,avgpu_m,"HLT",1,0,0,0,0,recLumi_m)
        SITeffresB_m=ROOT.calculateDataEfficiency(str(eosFile),args.dirEff,str(run),nMeasurements,goodLSlist[0]+1,goodLSlist[-1]+1,avgpu_m,"SIT",0,1,1,1,1,recLumi_m)#,mcDir+mcShapeSubDir+"MuStaEff/MC/probes.root",mcDir)
        SITeffresE_m=ROOT.calculateDataEfficiency(str(eosFile),args.dirEff,str(run),nMeasurements,goodLSlist[0]+1,goodLSlist[-1]+1,avgpu_m,"SIT",1,1,1,1,1,recLumi_m)#,mcDir+mcShapeSubDir+"MuStaEff/MC/probes.root",mcDir)
        StaeffresB_m=ROOT.calculateDataEfficiency(str(eosFile),args.dirEff,str(run),nMeasurements,goodLSlist[0]+1,goodLSlist[-1]+1,avgpu_m,"Sta",0,2,2,2,2,recLumi_m,mcDir+mcShapeSubDir+"MuStaEff/MC/probes.root",mcDir)
        StaeffresE_m=ROOT.calculateDataEfficiency(str(eosFile),args.dirEff,str(run),nMeasurements,goodLSlist[0]+1,goodLSlist[-1]+1,avgpu_m,"Sta",1,2,2,2,2,recLumi_m,mcDir+mcShapeSubDir+"MuStaEff/MC/probes.root",mcDir)
        
        #Zyield_m=ROOT.getZyield(str(eosFile),"h_yield_Z",str(run),LSchunks[chunk_m][0],LSchunks[chunk_m][-1])
        #Zyield_BB_m = ROOT.getZyield(str(eosFile),"h_yieldBB_Z",str(run),LSchunks[chunk_m][0],LSchunks[chunk_m][-1])
        #Zyield_EE_m = ROOT.getZyield(str(eosFile),"h_yieldEE_Z",str(run),LSchunks[chunk_m][0],LSchunks[chunk_m][-1])

        HLTeffB_m = HLTeffresB_m[0]
        HLTeffE_m = HLTeffresE_m[0]
        SITeffB_m = SITeffresB_m[0]
        SITeffE_m = SITeffresE_m[0]
        StaeffB_m = StaeffresB_m[0]
        StaeffE_m = StaeffresE_m[0]

        if StaeffresB_m[3] > staFitChi2Th or StaeffresB_m[4] > staFitChi2Th or StaeffB_m >= staFitEffThHi or StaeffB_m <= staFitEffThLo:
            StaeffB_m = prevStaEffB
            log.warning("======Bad fit might happen, origin eff = %f, with chi2 = %f, %f",StaeffresB_m[0],StaeffresB_m[3],StaeffresB_m[4])
        else:
            prevStaEffB = StaeffB_m

        if StaeffresE_m[3] > staFitChi2Th or StaeffresE_m[4] > staFitChi2Th or StaeffE_m >= staFitEffThHi or StaeffE_m <= staFitEffThLo:
            StaeffE_m = prevStaEffE
            log.warning("======Bad fit might happen, origin eff = %f, with chi2 = %f, %f",StaeffresE_m[0],StaeffresE_m[3],StaeffresE_m[4])
        else:
            prevStaEffE = StaeffE_m


        log.debug("======perMuonEff: %f, %f ,%f, %f, %f, %f",HLTeffB_m,HLTeffE_m,SITeffB_m,SITeffE_m,StaeffB_m,StaeffE_m)
        log.debug("======ZRawYield: %f",Zyield_m)

	#ZtoMuMu efficiency purely from data
        ZBBEff=(StaeffB_m*StaeffB_m * SITeffB_m*SITeffB_m * (1-(1-HLTeffB_m)*(1-HLTeffB_m)))
        ZBEEff=(StaeffB_m*StaeffE_m * SITeffB_m*SITeffE_m * (1-(1-HLTeffB_m)*(1-HLTeffE_m)))
        ZEEEff=(StaeffE_m*StaeffE_m * SITeffE_m*SITeffE_m * (1-(1-HLTeffE_m)*(1-HLTeffE_m)))

        #Statistic Uncertainties (low,high) error propagation 
        ZBBEffErr = [0.,0.]
        ZBEEffErr = [0.,0.]
        ZEEEffErr = [0.,0.]
        for i in (1,2):
            ZBBEffErr[i-1] = 2 * ZBBEff * np.sqrt( (StaeffresB_m[i]/StaeffB_m)**2 + (SITeffresB_m[i]/SITeffB_m)**2 + ((1-HLTeffB_m)/(1-(1-HLTeffB_m)**2)*HLTeffresB_m[i])**2 )
            ZEEEffErr[i-1] = 2 * ZEEEff * np.sqrt( (StaeffresE_m[i]/StaeffE_m)**2 + (SITeffresE_m[i]/SITeffE_m)**2 + ((1-HLTeffE_m)/(1-(1-HLTeffE_m)**2)*HLTeffresE_m[i])**2 )
            ZBEEffErr[i-1] = ZBEEff * np.sqrt( (StaeffresB_m[i]/StaeffB_m)**2 + (StaeffresE_m[i]/StaeffE_m)**2 + (SITeffresB_m[i]/SITeffB_m)**2 + (SITeffresE_m[i]/SITeffE_m)**2 + ((1-HLTeffE_m)/(1-(1-HLTeffB_m)*(1-HLTeffE_m))*HLTeffresB_m[i])**2 + ((1-HLTeffB_m)/(1-(1-HLTeffB_m)*(1-HLTeffE_m))*HLTeffresE_m[i])**2 )

	#ZtoMuMu efficiency correction as a parametrized function of pile-up
        ZBBEffCorr = f_ZBBEffCorr(avgpu_m)
	ZBEEffCorr = f_ZBEEffCorr(avgpu_m)
        ZEEEffCorr = f_ZEEEffCorr(avgpu_m)

	#ZtoMuMu efficiency after correction 
	ZMCEffBB = ZBBEff - ZBBEffCorr 
	ZMCEffBE = ZBEEff - ZBEEffCorr
	ZMCEffEE = ZEEEff - ZEEEffCorr
	

	#Multiply average frequency of each category with its efficiency
	ZMCEff = (ZMCEffBB * ZBBRate + ZMCEffBE * ZBERate + ZMCEffEE * ZEERate)/ (ZBBRate + ZBERate + ZEERate) 
        ZEff = (ZBBEff * ZBBRate + ZBEEff * ZBERate + ZEEEff * ZEERate)/ (ZBBRate + ZBERate + ZEERate)

        #Or better take the actual frequency?
        #ZMCEff = (ZMCEffBB*Zyield_BB_m + ZMCEffBE*(Zyield_m-Zyield_BB_m-Zyield_EE_m) + ZMCEffEE*Zyield_EE_m) / Zyield_m
        
        log.debug("======ZToMuMuEff: %f",ZMCEff)
        log.debug("======ZToMuMuEff: %f, %f ,%f, %f, %f, %f",ZMCEffBB, ZMCEffBE, ZMCEffEE, ZBBEff, ZBEEff, ZEEEff)

	#End products (about 1% fake rate)
        ZMCXSec  = Zyield_m*(1-ZfpRate)/(ZMCEff*recLumi_m)
        ZRateUnCorr  = Zyield_m*(1-ZfpRate)/(ZEff*timeWindow_m*deadtime_m)
        ZRate  = Zyield_m*(1-ZfpRate)/(ZMCEff*timeWindow_m*deadtime_m)
        log.debug("======ZMCXSec: %f",ZMCXSec)
        log.debug("======ZRate: %f",ZRate)

	#Variables to write in csv file
        fillarray.append(fill)

        beginTime.append(data_run.loc[data_run['ls'] == goodLSlist[0]]['time'].values[0])
        endTime.append(data_run.loc[data_run['ls'] == goodLSlist[-1]]['time'].values[0])
        ZrateUnCorr.append(ZRateUnCorr)
        Zrate.append(ZRate)
        instDel.append(delLumi_m/timeWindow_m)
        lumiDel.append(delLumi_m)
	pileUp.append(avgpu_m)
        ZyieldDel.append(Zyield_m*(1-ZfpRate)/(ZMCEff*deadtime_m))

	#Additional variables to write in efficiency csv file
        ZyieldRec.append(Zyield_m*(1-ZfpRate))
        lumiRec.append(recLumi_m)
        windowarray.append(timeWindow_m)
        deadTime.append(deadtime_m)
        beginLS.append(goodLSlist[0])
        endLS.append(goodLSlist[-1])
	#Efficiency related
    	HLTeffB.append(HLTeffB_m)
    	HLTeffE.append(HLTeffE_m)
        SITeffB.append(SITeffB_m)
        SITeffE.append(SITeffE_m)
        StaeffB.append(StaeffB_m)
        StaeffE.append(StaeffE_m)

        ZMCeff.append(ZMCEff)
        ZMCeffBB.append(ZMCEffBB)
        ZMCeffBE.append(ZMCEffBE)
        ZMCeffEE.append(ZMCEffEE)

        ZBBeff.append(ZBBEff)
        ZBEeff.append(ZBEEff)
        ZEEeff.append(ZEEEff)

        nMeasurements=nMeasurements+1

    ## Write Per Run CSV Files 
    print "Writing per Run CSV file"
    with open(args.dirCSV+'csvfile'+str(run)+'.csv','wb') as file:
        for c in range(0,nMeasurements):
		wline = str(int(fillarray[c]))+","+str(beginTime[c])+","+str(endTime[c])+","+str(Zrate[c])+","+str(instDel[c])+","+str(lumiDel[c])+","+str(ZyieldDel[c])+","+str(ZrateUnCorr[c]) 
                print(wline)
                file.write(wline + '\n')

    with open(args.dirCSV+'effcsvfile'+str(run)+'.csv','wb') as file:
        for c in range(0,nMeasurements):
		wline = str(int(fillarray[c]))+","+str(beginTime[c])+","+str(endTime[c])+","+str(Zrate[c])+","+str(instDel[c])+","+str(lumiDel[c])+","+str(ZyieldDel[c])+","+str(beginLS[c])+","+str(endLS[c])+","+str(lumiRec[c])+","+str(windowarray[c])+","+str(HLTeffB[c])+","+str(HLTeffE[c])+","+str(SITeffB[c])+","+str(SITeffE[c])+","+str(StaeffB[c])+","+str(StaeffE[c])+","+str(ZMCeff[c])+","+str(ZMCeffBB[c])+","+str(ZMCeffBE[c])+","+str(ZMCeffEE[c])+","+str(ZBBeff[c])+","+str(ZBEeff[c])+","+str(ZEEeff[c])+","+str(pileUp[c])
                file.write(wline + '\n')


## Write Big CSV File
print "Writing overall CSV file"
if args.writeSummaryCSV:
	rateFileList=sorted(glob.glob(args.dirCSV+'csvfile*.csv'))	
	with open(args.dirCSV+'Mergedcsvfile.csv','w') as file:
		file.write("fill,beginTime,endTime,ZRate,instDelLumi,delLumi,delZCount,ZrateUnCorr")
		file.write('\n')
		print "There are "+str(len(rateFileList))+" runs in the directory"
		for m in range(0,len(rateFileList)):						
			print "producing now csv file: "+rateFileList[m]
			try:
				iterF=open(rateFileList[m])
				lines=iterF.readlines()
				for line in lines:
					element=line.split(',')
					if element[3]=="nan" or element[3]=="0.0" or element[3]=="-0.0" or element[3]=="inf":
						continue
					file.write(line)
			except IOError as err:
    				print err.errno 
                                print err.strerror
				continue

        effFileList=sorted(glob.glob(args.dirCSV+'effcsvfile*.csv'))
	print "Starting to write efficiency files."	
        with open(args.dirCSV+'Mergedeffcsvfile.csv','wb') as fileTwo:
		fileTwo.write("fill,beginTime,endTime,ZRate,instDelLumi,delLumi,delZCount,beginLS,endLS,lumiRec,windowarray,HLTeffB,HLTeffE,SITeffB,SITeffE,,StaeffB,StaeffE,ZMCeff,ZMCeffBB,ZMCeffBE,ZMCeffEE,ZBBeff,ZBEeff,ZEEeff,pileUp")
		fileTwo.write('\n')
		for m in range(0,len(effFileList)):

			print "producing now eff csv file: "+rateFileList[m]
			try:
				iterF=open(effFileList[m])
				lines=iterF.readlines()
				for line in lines:
					element=line.split(',')
					if element[3]=="nan" or element[3]=="0.0" or element[3]=="-0.0" or element[3]=="inf":
						continue
					fileTwo.write(line)
                        except IOError as err:
				print err.errno 
                                print err.strerror
                                continue
